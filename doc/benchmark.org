* Benchmarks for wonko-client

** Conclusions

** Experiments and raw results
*** remote run
- will add numbers here when we do this.

*** local run
- JVM opts: ["-Xmx1g" "-Xms1g" "-server"]
- Number of calls per service request = 5
- Kafka config
 {"bootstrap.servers" "localhost:9092",
   "reconnect.backoff.ms" 50,
   "request.timeout.ms" 2,
   "retry.backoff.ms" 10,
   "linger.ms" 5,
   "timeout.ms" "10",
   "total.memory.bytes" (* 1024 1024 120),
   "metadata.fetch.timeout.ms" 10,
   "block.on.buffer.full" "false",
   "queue.enqueue.timeout.ms" 0,
   "compression.type" "gzip"}


| case            | submit rate (per s) | collector-rate | net submit-rate | wonko consume rate | con/prod ratio | queue size | tp size | Memory Impact (B) | CPU impact (%) | GC impact (%) |
|-----------------+---------------------+----------------+-----------------+--------------------+----------------+------------+---------+-------------------+----------------+---------------|
|                 |                     |            <r> |          #ERROR |                <r> | #ERROR         |        <r> |     <r> |               <r> |            <r> |               |
| nothing         |                   0 |              0 |               0 |                  0 | 0/0            |          - |       - |        95,068,224 |              0 |             0 |
| baseline (0ms)  |                   0 |              0 |               0 |                  0 | 0/0            |         10 |      10 |       110,196,776 |            2.0 |             0 |
| baseline (10ms) |                   0 |              0 |               0 |                  0 | 0/0            |         10 |      10 |       117,417,896 |            2.5 |             0 |
| with metrics    |                1000 |           2000 |            7000 |               6200 | 88.571429      |         10 |      10 |       423,000,000 |            8.5 |             0 |
| with metrics    |                2000 |           2000 |           12000 |              10300 | 85.833333      |         10 |      10 |       403,000,000 |           12.0 |               |
| with metrics    |                5000 |           2000 |           27000 |              13600 | 50.370370      |         10 |      10 |       406,000,000 |           14.0 |               |
| with metrics    |               10000 |           2000 |           52000 |              13600 | 26.153846      |         10 |      10 |       422,000,000 |           14.0 |               |
#+TBLFM: $4=($2*5)+$3::$6=(100*$5/$4)

**** commands used to run
#+begin_src clojure
;;warmup
(run true {:service-latency-ms 0
           :total-requests 50000
           :request-rate 1000
           :collector-interval-ms 1
           :collector-metrics-count 1000})

;; nothing
(run false {:service-latency-ms 0
           :total-requests 100
           :request-rate 10
           :collector-interval-ms 1
           :collector-metrics-count 1 })

;; baseline 0
(run false {:service-latency-ms 0
           :total-requests 10000
           :request-rate 1000
           :collector-interval-ms 1
           :collector-metrics-count 1000})

;; baseline 10
(run false {:service-latency-ms 10
           :total-requests 10000
           :request-rate 1000
           :collector-interval-ms 1
           :collector-metrics-count 1000})

;; with metrics at 1000rps
(run true {:service-latency-ms 10
           :total-requests 30000
           :request-rate 1000
           :collector-interval-ms 1000
           :collector-metrics-count 2000})

;; with metrics at 2000rps
(run true {:service-latency-ms 10
           :total-requests 60000
           :request-rate 2000
           :collector-interval-ms 1000
           :collector-metrics-count 2000})

;; with metrics at 5000rps
(run true {:service-latency-ms 0
           :total-requests 150000
           :request-rate 5000
           :collector-interval-ms 1000
           :collector-metrics-count 2000})

;; with metrics at 10000rps
(run true {:service-latency-ms 10
           :total-requests 300000
           :request-rate 15000
           :collector-interval-ms 1000
           :collector-metrics-count 2000})

#+end_src

*** local run version 2
Measuring send-sync and async times
**** measurements
| actual-request-rate | total-requests | throttled-rate | rejected-count | ast-999thPercentile | st-999thPercentile | st-median | st-75thPercentile | st-95thPercentile | st-99thPercentile | ast-median | ast-75thPercentile | ast-95thPercentile | ast-99thPercentile |
|---------------------+----------------+----------------+----------------+---------------------+--------------------+-----------+-------------------+-------------------+-------------------+------------+--------------------+--------------------+--------------------|
|                1645 |          10000 |           2000 |              0 |            592061.0 |           224851.0 |   52263.0 |           59644.0 |          139049.0 |          176341.0 |   204620.0 |           304276.0 |           412180.0 |           490408.0 |
|                2468 |          15000 |           3000 |              0 |            685714.0 |           190222.0 |   51775.0 |           57779.0 |          123107.0 |          167050.0 |   284585.0 |           426633.0 |           549806.0 |           609387.0 |
|                3278 |          20000 |           4000 |              0 |            887018.0 |           183955.0 |   51701.0 |           57402.0 |          107337.0 |          154336.0 |   340807.0 |           487466.0 |           633968.0 |           723875.0 |
|                4148 |          25000 |           5000 |              0 |            997566.0 |           211745.0 |   51385.0 |           56755.0 |          125644.0 |          161258.0 |   347007.0 |           494005.0 |           669176.0 |           811025.0 |
|                4970 |          30000 |           6000 |              0 |           1085657.0 |           239369.0 |   50214.0 |           55434.0 |          106049.0 |          166500.0 |   347853.0 |           508968.0 |           680257.0 |           855127.0 |
|                5810 |          35000 |           7000 |              0 |           1014315.0 |          1192795.0 |   49977.0 |           56164.0 |          123754.0 |          262995.0 |   330126.0 |           484050.0 |           685888.0 |           881378.0 |
|                6620 |          40000 |           8000 |              0 |           1747155.0 |           843696.0 |   49088.0 |           54430.0 |          109068.0 |          219006.0 |   368088.0 |           502889.0 |           705081.0 |           865689.0 |
|                4308 |          45000 |           9000 |              0 |           9868456.0 |          9792410.0 |   37505.0 |           48442.0 |           87083.0 |          285704.0 |   126117.0 |           214873.0 |           398697.0 |           656801.0 |
|                8338 |          50000 |          10000 |              0 |           1409603.0 |          1143992.0 |   48765.0 |           56200.0 |          170113.0 |          570600.0 |   270828.0 |           390935.0 |           641007.0 |           831266.0 |

**** test run code
#+begin_src clojure
(for [n [1000 2000 3000 4000 5000 6000 7000 8000 9000 10000]]
  (do
    (Thread/sleep 1000)
    (metrics-init)
    (reset! util/rejected-count 0)
    (let [total-requests (* 5 n)
	  throttled-fn (throttler/throttle-fn #(stream :some-api-call-again {:status 200 :boo 3004 :sdfca 49595 :asdfasdf 99032 :asdf "Sdf"} 999999999) n :second)
	  st (System/currentTimeMillis)]
      (doall
       (pmap (fn [_] (throttled-fn))
	     (range total-requests)))
      (let [et (System/currentTimeMillis)
	    exec-s (/ (- et st) 1000.0)]
	{:total-requests total-requests
	 :request-rate-per-second (long (/ total-requests exec-s))
	 :throttled-rate n
	 :rejected-count (long @util/rejected-count)
	 :send-sync-count (count (:values (bean (.getSnapshot send-sync-timer))))
	 :send-async-count (count (:values (bean (.getSnapshot send-sync-timer))))
	 :serialize-count (count (:values (bean (.getSnapshot kp/serialize-timer))))
	 :send-sync-time (select-keys (bean (.getSnapshot send-sync-timer)) [:median :75thPercentile :95thPercentile :99thPercentile :999thPercentile])
	 :send-async-time (select-keys (bean (.getSnapshot send-async-timer)) [:median :75thPercentile :95thPercentile :99thPercentile :999thPercentile])
	 :serialize-time (select-keys (bean (.getSnapshot kp/serialize-timer)) [:median :75thPercentile :95thPercentile :99thPercentile :999thPercentile])}))))
#+end_src

*** local run version 3
Ensure that there are no timeouts in kafka sending.
**** configs
#+begin_src clojure
(def without-timeouts-kafka-config
  {"bootstrap.servers" "localhost:9092",
   "linger.ms" 5,
   "total.memory.bytes" (* 1024 1024 120),
   "block.on.buffer.full" "true",
   "compression.type" "gzip"})

(defn init! []
  (init! "test"
         without-timeouts-kafka-config
         :thread-pool-size 10
         :queue-size 10
         :drop-on-reject? false))
#+end_src

*** disruptor wait strategy comparison

Notes:
- We'll go with BlockingWaitStrategy for wonko-client since it's the
  least risky of all, and most predictable in terms of CPU usage
- The next best strategy seems to be
  PhasedBackoffWaitStrategy/withLock with 1+1 config.
- CPU information is not useful because it includes production cost.
- SleepingWaitStrategy is extremely efficient (10x) but needs
  CPU. This could be used in scenarios where disruptor is a primary
  part of the application.

| rps  | strategy                    |       cpu (%) | config (Î¼s) | latency at 99.9%ile (ns) |
|------+-----------------------------+---------------+-------------+--------------------------|
|      |                             |           <r> |             |                          |
| 4300 | BlockingWaitStrategy        |            18 |           - |                   404475 |
| -    | SleepingWaitStrategy        | (constant) 50 |           - |                    46065 |
| -    | TimeoutBlockingWaitStrategy |             - |           1 |                        - |
| -    | PhasedBackoffWaitStrategy   |            20 |       10+10 |                   601707 |
| -    | PhasedBackoffWaitStrategy   |            20 |      100+10 |                   730032 |
| -    | PhasedBackoffWaitStrategy   |            19 |         1+1 |                   540837 |
| 9600 | PhasedBackoffWaitStrategy   |            90 |     0.5+0.5 |                  1124498 |
| 8000 | BlockingWaitStrategy        |            90 |           - |                  1230104 |

* Meta
** What kind of services are we looking to benchmark wonko-client for?
- Low latency services like Furtive and Eccentrica, that get over 1000
  requests per second, where request probably monitors about 5
  metrics. Roughly a couple of streams, counters and gauges.

** What questions are we looking to answer?
- What will the latency impact be?
- What will the memory requirement/impact be for such a service?
- What will the CPU requirement/impact be?
- What will the Network i/o impact be?
- What is the process of tuning wonko-client for performance or
  resource optimization?
- What are the available knobs/configs to tune performance? Are they
  sufficient?
- How do we tune wonko-client's performance for daemon/collector like
  processes that send a bunch of metrics in brief spikes or batches?

** What environment and h/w should the benchmarks be run on?
Typically, a production like environment. 4G RAM, 4 cores sound like a
reasonable configuration to run on without spending too much. We'll
run a real kafka instance in a separate machine/vm to emulate
reality.
